{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract model performance metrics\n",
    "\n",
    "In this notebook, we extract metrics to evaluate performance such as:\n",
    "\n",
    "1. Precision-recall\n",
    "2. Predicted probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from training_utils import get_X_y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to collect precision-recall results and predicted probabilities for binary models only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_curve_results(\n",
    "    model: LogisticRegression, df: pd.DataFrame, label: str, label_encoder: LabelEncoder\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Collect the precision-recall curve results from a model and dataset.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegression): loaded in logistic regression model to collect results from\n",
    "        df (pd.DataFrame): dataframe containing the data to apply model to\n",
    "        label (str): label with the class being predicted\n",
    "        label_encoder (LabelEncoder): encoder to transform the labels to integers\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with the PR curve results for that data and model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get X and y data for the model\n",
    "        X, y = get_X_y_data(df=df, label=label, shuffle_features=False)\n",
    "\n",
    "        assert all(\n",
    "            col in model.feature_names_in_ for col in X\n",
    "        ), \"Features in the model do not match the columns in the dataset\"\n",
    "\n",
    "        # Transform labels\n",
    "        y_encoded = label_encoder.transform(y)\n",
    "\n",
    "        # Ensure binary problem (this helper is for binary models only)\n",
    "        unique_labels = set(y_encoded)\n",
    "        assert (\n",
    "            len(unique_labels) == 2\n",
    "        ), f\"Expected binary labels after encoding, got classes: {sorted(list(unique_labels))}\"\n",
    "\n",
    "        # Ensure model supports predict_proba and returns expected shape\n",
    "        if not hasattr(model, \"predict_proba\"):\n",
    "            raise AssertionError(\n",
    "                \"Model does not implement predict_proba required for PR curve\"\n",
    "            )\n",
    "\n",
    "        y_proba = model.predict_proba(X)\n",
    "        if (\n",
    "            not hasattr(y_proba, \"shape\")\n",
    "            or len(y_proba.shape) != 2\n",
    "            or y_proba.shape[1] < 2\n",
    "        ):\n",
    "            raise AssertionError(\n",
    "                f\"predict_proba returned unexpected shape: {getattr(y_proba, 'shape', None)}\"\n",
    "            )\n",
    "\n",
    "        y_scores = y_proba[:, 1]\n",
    "\n",
    "        assert len(y_scores) == len(\n",
    "            y_encoded\n",
    "        ), \"Length mismatch between predicted scores and labels\"\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_encoded, y_scores)\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise AssertionError(\n",
    "            f\"Failed to compute PR curve for label '{label}': {e}\"\n",
    "        ) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_probabilities(\n",
    "    model: LogisticRegression, df: pd.DataFrame, label: str, label_encoder: LabelEncoder\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Collect predicted probabilities per single-cell from the model and dataset.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegression): loaded in logistic regression model to collect results from\n",
    "        df (pd.DataFrame): dataframe containing the data to apply model to\n",
    "        label (str): label with the class being predicted\n",
    "        label_encoder (LabelEncoder): encoder to transform the labels to integers\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with the predicted probabilities per single-cell\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate required metadata columns exist for output\n",
    "        for col in (\"Metadata_treatment\", \"Metadata_heart_number\"):\n",
    "            assert col in df.columns, f\"Required column '{col}' not found in dataframe\"\n",
    "\n",
    "        # Set treatment and heart number metadata to include in output\n",
    "        metadata_treatment = df[\"Metadata_treatment\"].values\n",
    "        metadata_heart_number = df[\"Metadata_heart_number\"].values\n",
    "\n",
    "        # Get X and y for the model\n",
    "        X, y = get_X_y_data(df=df, label=label, shuffle_features=False)\n",
    "\n",
    "        # Ensure model features match dataset columns\n",
    "        assert all(\n",
    "            col in model.feature_names_in_ for col in X\n",
    "        ), \"Features in the model do not match the columns in the dataset\"\n",
    "\n",
    "        # Encode labels and ensure binary problem (this helper is for binary models only)\n",
    "        y_encoded = label_encoder.transform(y)\n",
    "        unique_labels = set(y_encoded)\n",
    "        assert (\n",
    "            len(unique_labels) == 2\n",
    "        ), f\"Expected binary labels after encoding, got classes: {sorted(list(unique_labels))}\"\n",
    "\n",
    "        # Ensure model supports predict_proba and returns expected shape\n",
    "        if not hasattr(model, \"predict_proba\"):\n",
    "            raise AssertionError(\n",
    "                \"Model does not implement predict_proba required to get predicted probabilities\"\n",
    "            )\n",
    "\n",
    "        y_proba = model.predict_proba(X)\n",
    "        if not hasattr(y_proba, \"shape\") or len(y_proba.shape) != 2:\n",
    "            raise AssertionError(\n",
    "                f\"predict_proba returned unexpected shape: {getattr(y_proba, 'shape', None)}\"\n",
    "            )\n",
    "        if y_proba.shape[1] < 2:\n",
    "            raise AssertionError(\n",
    "                f\"predict_proba returned fewer than 2 class probabilities: shape {y_proba.shape}\"\n",
    "            )\n",
    "        if y_proba.shape[0] != len(X):\n",
    "            raise AssertionError(\n",
    "                f\"predict_proba returned {y_proba.shape[0]} rows but expected {len(X)}\"\n",
    "            )\n",
    "\n",
    "        # Use probability for the positive class\n",
    "        y_scores = y_proba[:, 1]\n",
    "\n",
    "        # Consistency checks\n",
    "        assert len(y_scores) == len(\n",
    "            y_encoded\n",
    "        ), \"Length mismatch between predicted probabilities and labels\"\n",
    "        assert len(metadata_treatment) == len(y_scores) and len(\n",
    "            metadata_heart_number\n",
    "        ) == len(\n",
    "            y_scores\n",
    "        ), \"Length mismatch between metadata columns and predicted probabilities\"\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"actual_label\": y,\n",
    "                \"predicted_probability\": y_scores,\n",
    "                \"Metadata_treatment\": metadata_treatment,\n",
    "                \"Metadata_heart_number\": metadata_heart_number,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise AssertionError(\n",
    "            f\"Failed to compute predicted probabilities for label '{label}': {e}\"\n",
    "        ) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plate to process (original or redo)\n",
    "plate_to_process = \"redo\"  # options: \"original\" or \"redo\"\n",
    "\n",
    "# Directory with the training and testing datasets per plate (or combined per batch)\n",
    "data_dir = pathlib.Path(f\"data_splits/{plate_to_process}_DMSO_plate\")\n",
    "\n",
    "# Directory with the trained models\n",
    "model_dir = pathlib.Path(f\"models/{plate_to_process}_DMSO_plate\")\n",
    "\n",
    "# Directory with encoder\n",
    "encoder_dir = pathlib.Path(f\"encoder_results/{plate_to_process}_DMSO_plate\")\n",
    "\n",
    "# Output directory the performance metrics\n",
    "performance_metrics_dir = pathlib.Path(\n",
    "    f\"performance_metrics/{plate_to_process}_DMSO_plate\"\n",
    ")\n",
    "performance_metrics_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Label being predicted\n",
    "label = \"Metadata_cell_type\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary with all relevant paths per plate to extract metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: model_HCM\n",
      "  training_data: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/data_splits/redo_DMSO_plate/model_HCM/downsample_training_split.parquet\n",
      "  testing_data: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/data_splits/redo_DMSO_plate/model_HCM/testing_split.parquet\n",
      "  holdout_data: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/data_splits/redo_DMSO_plate/model_HCM/holdout_split.parquet\n",
      "  final_model: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/models/redo_DMSO_plate/model_HCM_final_downsample.joblib\n",
      "  shuffled_model: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/models/redo_DMSO_plate/model_HCM_shuffled_downsample.joblib\n",
      "  encoder_result: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/encoder_results/redo_DMSO_plate/label_encoder_global.joblib\n",
      "Model: model_all_hearts\n",
      "  training_data: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/data_splits/redo_DMSO_plate/model_all_hearts/downsample_training_split.parquet\n",
      "  testing_data: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/data_splits/redo_DMSO_plate/model_all_hearts/testing_split.parquet\n",
      "  holdout_data: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/data_splits/redo_DMSO_plate/model_all_hearts/holdout_split.parquet\n",
      "  final_model: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/models/redo_DMSO_plate/model_all_hearts_final_downsample.joblib\n",
      "  shuffled_model: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/models/redo_DMSO_plate/model_all_hearts_shuffled_downsample.joblib\n",
      "  encoder_result: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/encoder_results/redo_DMSO_plate/label_encoder_global.joblib\n",
      "Model: model_DCM\n",
      "  training_data: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/data_splits/redo_DMSO_plate/model_DCM/downsample_training_split.parquet\n",
      "  testing_data: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/data_splits/redo_DMSO_plate/model_DCM/testing_split.parquet\n",
      "  holdout_data: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/data_splits/redo_DMSO_plate/model_DCM/holdout_split.parquet\n",
      "  final_model: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/models/redo_DMSO_plate/model_DCM_final_downsample.joblib\n",
      "  shuffled_model: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/models/redo_DMSO_plate/model_DCM_shuffled_downsample.joblib\n",
      "  encoder_result: /home/jenna/predicting_cardiac_fibrosis_etiologies/5.machine_learning/encoder_results/redo_DMSO_plate/label_encoder_global.joblib\n"
     ]
    }
   ],
   "source": [
    "# Extract model names from model filenames\n",
    "model_names = set(\n",
    "    f.stem.replace(\"_final_downsample\", \"\")\n",
    "    for f in model_dir.glob(\"*_final_downsample.joblib\")\n",
    ")\n",
    "\n",
    "# Create a nested dictionary with info per model\n",
    "models_dict = {}\n",
    "for model in model_names:\n",
    "    models_dict[model] = {\n",
    "        \"training_data\": pathlib.Path(\n",
    "            data_dir / model / \"downsample_training_split.parquet\"\n",
    "        ).resolve(strict=True),\n",
    "        \"testing_data\": pathlib.Path(\n",
    "            data_dir / model / \"testing_split.parquet\"\n",
    "        ).resolve(strict=True),\n",
    "        \"holdout_data\": pathlib.Path(\n",
    "            data_dir / model / \"holdout_split.parquet\"\n",
    "        ).resolve(strict=True),\n",
    "        \"final_model\": pathlib.Path(\n",
    "            model_dir / f\"{model}_final_downsample.joblib\"\n",
    "        ).resolve(strict=True),\n",
    "        \"shuffled_model\": pathlib.Path(\n",
    "            model_dir / f\"{model}_shuffled_downsample.joblib\"\n",
    "        ).resolve(strict=True),\n",
    "        \"encoder_result\": pathlib.Path(\n",
    "            encoder_dir / \"label_encoder_global.joblib\"\n",
    "        ).resolve(strict=True),\n",
    "    }\n",
    "\n",
    "# Print out dictionary keys and paths for verification\n",
    "for model, paths in models_dict.items():\n",
    "    lines = [f\"Model: {model}\"] + [f\"  {key}: {path}\" for key, path in paths.items()]\n",
    "    print(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: model_HCM\n",
      "  train (3): 2, 7, 46\n",
      "  test (3): 2, 7, 46\n",
      "  holdout (3): 2, 7, 46\n",
      "Model: model_all_hearts\n",
      "  train (5): 2, 7, 23, 25, 46\n",
      "  test (5): 2, 7, 23, 25, 46\n",
      "  holdout (5): 2, 7, 23, 25, 46\n",
      "Model: model_DCM\n",
      "  train (4): 2, 7, 23, 25\n",
      "  test (4): 2, 7, 23, 25\n",
      "  holdout (4): 2, 7, 23, 25\n"
     ]
    }
   ],
   "source": [
    "# For each model, print unique Metadata_heart_number per data split\n",
    "for model_name, paths in models_dict.items():\n",
    "    # load datasets\n",
    "    downsample_train_df = pd.read_parquet(paths[\"training_data\"])\n",
    "    test_df = pd.read_parquet(paths[\"testing_data\"])\n",
    "    holdout_df = pd.read_parquet(paths[\"holdout_data\"])\n",
    "\n",
    "    # collect unique heart numbers per split and print\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for split_name, df in [\n",
    "        (\"train\", downsample_train_df),\n",
    "        (\"test\", test_df),\n",
    "        (\"holdout\", holdout_df),\n",
    "    ]:\n",
    "        if \"Metadata_heart_number\" not in df.columns:\n",
    "            print(f\"  {split_name}: Metadata_heart_number column not found\")\n",
    "            continue\n",
    "        unique_hearts = pd.Series(df[\"Metadata_heart_number\"].dropna().unique())\n",
    "        # Print unique heart numbers horizontally, sorted and compact\n",
    "        hearts = sorted(unique_hearts.tolist())\n",
    "        if len(hearts) == 0:\n",
    "            print(f\"  {split_name} (0): None\")\n",
    "        else:\n",
    "            hearts_str = \", \".join(str(h) for h in hearts)\n",
    "            print(f\"  {split_name} ({len(hearts)}): {hearts_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model key: model_all_hearts\n",
      "Held-out Metadata_Well values and cell counts per Metadata_heart_number for DMSO treatment:\n",
      " Metadata_heart_number heldout_wells cells_per_well_list well_cell_counts\n",
      "                     2         [D05]               [204]     {'D05': 204}\n",
      "                     7         [G07]                [79]      {'G07': 79}\n",
      "                    23         [F09]               [126]     {'F09': 126}\n",
      "                    25         [B06]               [137]     {'B06': 137}\n",
      "                    46         [E11]               [294]     {'E11': 294}\n"
     ]
    }
   ],
   "source": [
    "# Find the model key that corresponds to the \"all hearts\" model\n",
    "model_key = next(\n",
    "    (k for k in models_dict.keys() if \"all\" in k.lower() and \"heart\" in k.lower()),\n",
    "    None,\n",
    ")\n",
    "\n",
    "if model_key is None:\n",
    "    raise KeyError(\n",
    "        f\"No model key matching 'all' and 'heart' found. Available keys: {list(models_dict.keys())}\"\n",
    "    )\n",
    "\n",
    "# Load holdout dataset for that model\n",
    "holdout_path = models_dict[model_key][\"holdout_data\"]\n",
    "holdout_df = pd.read_parquet(holdout_path)\n",
    "\n",
    "# Filter to DMSO treatment only and drop rows missing required columns\n",
    "dmso_holdout = holdout_df.loc[\n",
    "    (holdout_df[\"Metadata_treatment\"] == \"DMSO\")\n",
    "    & holdout_df[\"Metadata_heart_number\"].notna()\n",
    "    & holdout_df[\"Metadata_Well\"].notna()\n",
    "].copy()\n",
    "\n",
    "# Compute number of rows/cells per (heart, well)\n",
    "cells_per_well = (\n",
    "    dmso_holdout.groupby([\"Metadata_heart_number\", \"Metadata_Well\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"n_cells\")\n",
    ")\n",
    "\n",
    "# For each heart, collect sorted list of wells and corresponding counts\n",
    "wells_per_heart = (\n",
    "    cells_per_well.sort_values([\"Metadata_heart_number\", \"Metadata_Well\"])\n",
    "    .groupby(\"Metadata_heart_number\")\n",
    "    .agg(\n",
    "        heldout_wells=(\"Metadata_Well\", lambda s: sorted(list(s))),\n",
    "        cells_per_well_list=(\"n_cells\", lambda s: list(s)),\n",
    "        wells_with_counts=(\n",
    "            \"n_cells\",\n",
    "            lambda s, idx=None: None,\n",
    "        ),  # placeholder column removed below\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Replace wells_with_counts with a mapping from well -> count for clarity\n",
    "wells_per_heart = wells_per_heart.drop(columns=[\"wells_with_counts\"])\n",
    "wells_per_heart[\"well_cell_counts\"] = wells_per_heart[\"Metadata_heart_number\"].map(\n",
    "    lambda h: dict(\n",
    "        cells_per_well.loc[\n",
    "            cells_per_well[\"Metadata_heart_number\"] == h, [\"Metadata_Well\", \"n_cells\"]\n",
    "        ]\n",
    "        .set_index(\"Metadata_Well\")[\"n_cells\"]\n",
    "        .to_dict()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Model key: {model_key}\")\n",
    "print(\n",
    "    \"Held-out Metadata_Well values and cell counts per Metadata_heart_number for DMSO treatment:\"\n",
    ")\n",
    "print(wells_per_heart.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract metrics from the data splits applied to their respective models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: model_HCM\n",
      "MODEL_HCM | model_HCM | final | train → Done\n",
      "MODEL_HCM | model_HCM | shuffled | train → Done\n",
      "MODEL_HCM | model_HCM | final | test → Done\n",
      "MODEL_HCM | model_HCM | shuffled | test → Done\n",
      "MODEL_HCM | model_HCM | final | holdout → Done\n",
      "MODEL_HCM | model_HCM | shuffled | holdout → Done\n",
      "Processing model: model_all_hearts\n",
      "MODEL_ALL_HEARTS | model_all_hearts | final | train → Done\n",
      "MODEL_ALL_HEARTS | model_all_hearts | shuffled | train → Done\n",
      "MODEL_ALL_HEARTS | model_all_hearts | final | test → Done\n",
      "MODEL_ALL_HEARTS | model_all_hearts | shuffled | test → Done\n",
      "MODEL_ALL_HEARTS | model_all_hearts | final | holdout → Done\n",
      "MODEL_ALL_HEARTS | model_all_hearts | shuffled | holdout → Done\n",
      "Processing model: model_DCM\n",
      "MODEL_DCM | model_DCM | final | train → Done\n",
      "MODEL_DCM | model_DCM | shuffled | train → Done\n",
      "MODEL_DCM | model_DCM | final | test → Done\n",
      "MODEL_DCM | model_DCM | shuffled | test → Done\n",
      "MODEL_DCM | model_DCM | final | holdout → Done\n",
      "MODEL_DCM | model_DCM | shuffled | holdout → Done\n",
      "(32994, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>model_type</th>\n",
       "      <th>dataset</th>\n",
       "      <th>model_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>final</td>\n",
       "      <td>train</td>\n",
       "      <td>model_HCM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>final</td>\n",
       "      <td>train</td>\n",
       "      <td>model_HCM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision  recall model_type dataset model_name\n",
       "0   0.500000     1.0      final   train  model_HCM\n",
       "1   0.500203     1.0      final   train  model_HCM"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize results list\n",
    "test_train_pr_results = []\n",
    "test_train_probability_results = []\n",
    "\n",
    "# Run through each model and get the PR results\n",
    "for model_name, paths in models_dict.items():\n",
    "    # Load the models and data\n",
    "    final_model = load(paths[\"final_model\"])\n",
    "    shuffled_model = load(paths[\"shuffled_model\"])\n",
    "    label_encoder = load(paths[\"encoder_result\"])\n",
    "    downsample_train_df = pd.read_parquet(paths[\"training_data\"])\n",
    "    test_df = pd.read_parquet(paths[\"testing_data\"])\n",
    "    holdout_df = pd.read_parquet(paths[\"holdout_data\"])\n",
    "\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "\n",
    "    # Set dictionary with the data splits\n",
    "    datasets = {\"train\": downsample_train_df, \"test\": test_df, \"holdout\": holdout_df}\n",
    "\n",
    "    # Loop through both datasets and models\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        for model_type, model in [(\"final\", final_model), (\"shuffled\", shuffled_model)]:\n",
    "            # Get per-sample predicted probabilities\n",
    "            prob_df = get_predicted_probabilities(\n",
    "                model=model, df=dataset, label=label, label_encoder=label_encoder\n",
    "            )\n",
    "            prob_df[\"model_type\"] = model_type\n",
    "            prob_df[\"dataset\"] = dataset_name\n",
    "            prob_df[\"model_name\"] = model_name\n",
    "            test_train_probability_results.append(prob_df)\n",
    "\n",
    "            # Get PR curve results (global)\n",
    "            pr_df = get_pr_curve_results(\n",
    "                model=model, df=dataset, label=label, label_encoder=label_encoder\n",
    "            )\n",
    "            pr_df[\"model_type\"] = model_type\n",
    "            pr_df[\"dataset\"] = dataset_name\n",
    "            pr_df[\"model_name\"] = model_name\n",
    "            test_train_pr_results.append(pr_df)\n",
    "\n",
    "            print(\n",
    "                f\"{model_name.upper()} | {model_name} | {model_type} | {dataset_name} → Done\"\n",
    "            )\n",
    "\n",
    "# Combine all results into one dataframe\n",
    "all_models_pr_results_df = pd.concat(test_train_pr_results, ignore_index=True)\n",
    "all_models_probabilities_df = pd.concat(\n",
    "    test_train_probability_results, ignore_index=True\n",
    ")\n",
    "\n",
    "# Save the results\n",
    "all_models_pr_results_df.to_parquet(\n",
    "    performance_metrics_dir / \"all_models_pr_curve_results.parquet\", index=False\n",
    ")\n",
    "all_models_probabilities_df.to_parquet(\n",
    "    performance_metrics_dir / \"all_models_predicted_probabilities.parquet\", index=False\n",
    ")\n",
    "\n",
    "# Check output\n",
    "print(all_models_pr_results_df.shape)\n",
    "all_models_pr_results_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract performance from the multi-class model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in final and shuffled model for multi-class models\n",
    "final_model = load(pathlib.Path(model_dir / \"model_all_hearts_final_multiclass.joblib\"))\n",
    "shuffled_model = load(\n",
    "    pathlib.Path(model_dir / \"model_all_hearts_shuffled_multiclass.joblib\")\n",
    ")\n",
    "\n",
    "# Set paths to data splits\n",
    "training_data_path = pathlib.Path(\n",
    "    data_dir\n",
    "    / \"model_all_hearts\"\n",
    "    / \"training_split.parquet\"  # Did not downsample for the multi-class model\n",
    ").resolve(strict=True)\n",
    "testing_data_path = pathlib.Path(\n",
    "    data_dir / \"model_all_hearts\" / \"testing_split.parquet\"\n",
    ").resolve(strict=True)\n",
    "holdout_data_path = pathlib.Path(\n",
    "    data_dir / \"model_all_hearts\" / \"holdout_split.parquet\"\n",
    ").resolve(strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_all_hearts_multiclass | final | train | 0 → Done\n",
      "model_all_hearts_multiclass | final | train | 1 → Done\n",
      "model_all_hearts_multiclass | final | train | 2 → Done\n",
      "model_all_hearts_multiclass | shuffled | train | 0 → Done\n",
      "model_all_hearts_multiclass | shuffled | train | 1 → Done\n",
      "model_all_hearts_multiclass | shuffled | train | 2 → Done\n",
      "model_all_hearts_multiclass | final | test | 0 → Done\n",
      "model_all_hearts_multiclass | final | test | 1 → Done\n",
      "model_all_hearts_multiclass | final | test | 2 → Done\n",
      "model_all_hearts_multiclass | shuffled | test | 0 → Done\n",
      "model_all_hearts_multiclass | shuffled | test | 1 → Done\n",
      "model_all_hearts_multiclass | shuffled | test | 2 → Done\n",
      "model_all_hearts_multiclass | final | holdout | 0 → Done\n",
      "model_all_hearts_multiclass | final | holdout | 1 → Done\n",
      "model_all_hearts_multiclass | final | holdout | 2 → Done\n",
      "model_all_hearts_multiclass | shuffled | holdout | 0 → Done\n",
      "model_all_hearts_multiclass | shuffled | holdout | 1 → Done\n",
      "model_all_hearts_multiclass | shuffled | holdout | 2 → Done\n",
      "Saved PR results → performance_metrics/redo_DMSO_plate/multi_class_pr_results.parquet\n",
      "Saved heart accuracy → performance_metrics/redo_DMSO_plate/multi_class_heart_accuracy.parquet\n",
      "Saved cell probabilities → performance_metrics/redo_DMSO_plate/multi_class_cell_probabilities.parquet\n"
     ]
    }
   ],
   "source": [
    "# Initialize results list\n",
    "multi_class_pr_results = []\n",
    "heart_accuracy_results = []\n",
    "cell_probs_results = []\n",
    "\n",
    "# Load in label encoder for multi-class model\n",
    "label_encoder = load(pathlib.Path(encoder_dir / \"label_encoder_multi-class.joblib\"))\n",
    "\n",
    "# Set updated label variable\n",
    "label = \"Metadata_heart_failure_type\"\n",
    "\n",
    "for dataset_name, data_path in [\n",
    "    (\"train\", training_data_path),\n",
    "    (\"test\", testing_data_path),\n",
    "    (\"holdout\", holdout_data_path),\n",
    "]:\n",
    "    # Load dataset\n",
    "    df = pd.read_parquet(data_path)\n",
    "    df[label] = df[label].fillna(\"Healthy\")\n",
    "\n",
    "    # Get X and y for the model\n",
    "    X, y = get_X_y_data(df=df, label=label, shuffle_features=False)\n",
    "    y_encoded = label_encoder.transform(y)\n",
    "\n",
    "    for model_type, model in [(\"final\", final_model), (\"shuffled\", shuffled_model)]:\n",
    "        y_pred_proba = model.predict_proba(X)\n",
    "        y_pred_label = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "        # --- Save predicted probabilities per cell ---\n",
    "        cell_probs_results.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"dataset\": dataset_name,\n",
    "                    \"model_type\": model_type,\n",
    "                    \"model_name\": \"model_all_hearts_multiclass\",\n",
    "                    \"Metadata_heart_number\": df[\"Metadata_heart_number\"],\n",
    "                    \"true_label\": y_encoded,\n",
    "                    \"predicted_label\": y_pred_label,\n",
    "                    **{\n",
    "                        f\"proba_class_{i}\": y_pred_proba[:, i]\n",
    "                        for i in range(y_pred_proba.shape[1])\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # --- Compute PR curve per class ---\n",
    "        for class_index, class_label in enumerate(model.classes_):\n",
    "            y_true_binary = (y_encoded == class_index).astype(int)\n",
    "            y_scores = y_pred_proba[:, class_index]\n",
    "\n",
    "            precision, recall, _ = precision_recall_curve(y_true_binary, y_scores)\n",
    "\n",
    "            pr_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"model_type\": model_type,\n",
    "                    \"dataset\": dataset_name,\n",
    "                    \"model_name\": \"model_all_hearts_multiclass\",\n",
    "                    \"class_label\": class_label,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            multi_class_pr_results.append(pr_df)\n",
    "\n",
    "            # --- Compute per-heart × treatment accuracy ---\n",
    "            for (heart, treatment), group_df in df.groupby(\n",
    "                [\"Metadata_heart_number\", \"Metadata_treatment\"]\n",
    "            ):\n",
    "                mask = X.index.isin(group_df.index)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                heart_treatment_acc = (y_pred_label[mask] == y_encoded[mask]).mean()\n",
    "                heart_accuracy_results.append(\n",
    "                    {\n",
    "                        \"dataset\": dataset_name,\n",
    "                        \"model_type\": model_type,\n",
    "                        \"model_name\": \"model_all_hearts_multiclass\",\n",
    "                        \"heart_number\": heart,\n",
    "                        \"treatment\": treatment,\n",
    "                        \"accuracy\": heart_treatment_acc,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                f\"model_all_hearts_multiclass | {model_type} | {dataset_name} | {class_label} → Done\"\n",
    "            )\n",
    "\n",
    "# Save PR results\n",
    "pr_results_df = pd.concat(multi_class_pr_results, ignore_index=True)\n",
    "pr_results_df.to_parquet(\n",
    "    performance_metrics_dir / \"multi_class_pr_results.parquet\", index=False\n",
    ")\n",
    "\n",
    "# Save heart accuracy\n",
    "heart_accuracy_df = pd.DataFrame(heart_accuracy_results)\n",
    "heart_accuracy_df.to_parquet(\n",
    "    performance_metrics_dir / \"multi_class_heart_accuracy.parquet\", index=False\n",
    ")\n",
    "\n",
    "# Save per-cell predicted probabilities\n",
    "cell_probs_df = pd.concat(cell_probs_results, ignore_index=True)\n",
    "cell_probs_df.to_parquet(\n",
    "    performance_metrics_dir / \"multi_class_cell_probabilities.parquet\", index=False\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Saved PR results → {performance_metrics_dir / 'multi_class_pr_results.parquet'}\"\n",
    ")\n",
    "print(\n",
    "    f\"Saved heart accuracy → {performance_metrics_dir / 'multi_class_heart_accuracy.parquet'}\"\n",
    ")\n",
    "print(\n",
    "    f\"Saved cell probabilities → {performance_metrics_dir / 'multi_class_cell_probabilities.parquet'}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fibrosis_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
