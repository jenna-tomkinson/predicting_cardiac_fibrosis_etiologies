{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bbd55ef",
   "metadata": {},
   "source": [
    "# Train three models with subsequent shuffled baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252a12a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from sklearn.base import clone\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import parallel_backend\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from training_utils import downsample_data, get_X_y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3878b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 training datasets.\n",
      "Loading dataset: model_HCM\n",
      "Loading dataset: model_all_hearts\n",
      "Loading dataset: model_DCM\n"
     ]
    }
   ],
   "source": [
    "# Set numpy seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set if processing redo plate (true) or original plate (false)\n",
    "redo_plate = True\n",
    "\n",
    "# Metadata column used for prediction class\n",
    "label = \"Metadata_cell_type\"\n",
    "\n",
    "if redo_plate:\n",
    "    # Path to training/testing datasets for redo plate\n",
    "    training_data_path = pathlib.Path(\"./data_splits/redo_DMSO_plate\")\n",
    "    # Directories for outputs\n",
    "    model_dir = pathlib.Path(\"./models/redo_DMSO_plate\")\n",
    "    model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    encoder_dir = pathlib.Path(\"./encoder_results/redo_DMSO_plate\")\n",
    "    encoder_dir.mkdir(exist_ok=True, parents=True)\n",
    "else:  # process original plate\n",
    "    # Path to training/testing datasets for original plate\n",
    "    training_data_path = pathlib.Path(\"./data_splits/original_DMSO_plate\")\n",
    "    # Directories for outputs\n",
    "    model_dir = pathlib.Path(\"./models/original_DMSO_plate\")\n",
    "    model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    encoder_dir = pathlib.Path(\"./encoder_results/original_DMSO_plate\")\n",
    "    encoder_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Find all training datasets\n",
    "training_files = list(training_data_path.rglob(\"training_split.parquet\"))\n",
    "\n",
    "print(f\"Found {len(training_files)} training datasets.\")\n",
    "\n",
    "# Dictionary to store loaded training datasets\n",
    "training_dfs = {}\n",
    "\n",
    "# Loop through and load each training dataset\n",
    "for training_file in training_files:\n",
    "    dataset_name = training_file.parent.name  # Use parent folder name as key\n",
    "    print(f\"Loading dataset: {dataset_name}\")  # only print the model/folder name\n",
    "\n",
    "    train_df = pd.read_parquet(training_file)\n",
    "    training_dfs[dataset_name] = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381289d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file created at data_splits/redo_DMSO_plate/model_HCM/downsample_training_split.parquet with 2464 rows.\n",
      "(2464, 1030)\n",
      "Failing    1232\n",
      "Healthy    1232\n",
      "Name: Metadata_cell_type, dtype: int64\n",
      "Parquet file created at data_splits/redo_DMSO_plate/model_all_hearts/downsample_training_split.parquet with 2464 rows.\n",
      "(2464, 1030)\n",
      "Failing    1232\n",
      "Healthy    1232\n",
      "Name: Metadata_cell_type, dtype: int64\n",
      "Parquet file created at data_splits/redo_DMSO_plate/model_DCM/downsample_training_split.parquet with 2228 rows.\n",
      "(2228, 1030)\n",
      "Failing    1114\n",
      "Healthy    1114\n",
      "Name: Metadata_cell_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Loop through and downsample each loaded training dataset\n",
    "for dataset_name, train_df in training_dfs.items():\n",
    "    # Downsample to the smallest class\n",
    "    downsample_df = downsample_data(data=train_df, label=label)\n",
    "\n",
    "    # Replace and store the downsampled dataframe\n",
    "    training_dfs[dataset_name] = downsample_df\n",
    "\n",
    "    # Export as a new parquet with just the rows after downsampling per model (not indices)\n",
    "    output_file = (\n",
    "        training_data_path / dataset_name / \"downsample_training_split.parquet\"\n",
    "    )\n",
    "    downsample_df.to_parquet(output_file, index=False)\n",
    "\n",
    "    print(f\"Parquet file created at {output_file} with {downsample_df.shape[0]} rows.\")\n",
    "    print(downsample_df.shape)\n",
    "    print(downsample_df[label].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d168139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Class Mapping:\n",
      "{'Failing': 0, 'Healthy': 1}\n"
     ]
    }
   ],
   "source": [
    "# Collect all unique labels across all datasets\n",
    "all_labels = set()\n",
    "for dataset_name, train_df in training_dfs.items():\n",
    "    all_labels.update(train_df[label].unique())\n",
    "\n",
    "# Fit the LabelEncoder on the combined set of all labels\n",
    "le = LabelEncoder()\n",
    "le.fit(list(all_labels))\n",
    "\n",
    "# Save the global label encoder for consistency\n",
    "dump(le, encoder_dir / \"label_encoder_global.joblib\")\n",
    "\n",
    "# Print the global class mapping\n",
    "class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Global Class Mapping:\")\n",
    "print(class_mapping)\n",
    "\n",
    "# New dictionary to hold final training data with X and y\n",
    "training_data = {}\n",
    "\n",
    "# Process each dataset to get X and y\n",
    "for dataset_name, train_df in training_dfs.items():\n",
    "    # Non-shuffled data\n",
    "    X_train, y_train = get_X_y_data(df=train_df, label=label, shuffle_features=False)\n",
    "    y_train_encoded = le.transform(y_train)\n",
    "\n",
    "    # Shuffled data\n",
    "    X_shuffled_train, y_shuffled_train = get_X_y_data(\n",
    "        df=train_df, label=label, shuffle_features=True\n",
    "    )\n",
    "    y_shuffled_train_encoded = le.transform(y_shuffled_train)\n",
    "\n",
    "    # Store X and y in the dictionary\n",
    "    training_data[dataset_name] = {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train_encoded,\n",
    "        \"X_shuffled_train\": X_shuffled_train,\n",
    "        \"y_shuffled_train\": y_shuffled_train_encoded,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6007141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folds for k-fold cross validation (default is 5, shuffle=True)\n",
    "straified_k_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Set Logistic Regression model parameters (use default for max_iter)\n",
    "logreg_params = {\n",
    "    \"penalty\": \"elasticnet\",\n",
    "    \"solver\": \"saga\",\n",
    "    \"max_iter\": 1000,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 0,\n",
    "    \"class_weight\": \"balanced\",\n",
    "}\n",
    "\n",
    "# Define the hyperparameter search space for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    \"C\": np.logspace(-2, 1, 7),  # values from 0.01 to 10\n",
    "    \"l1_ratio\": np.linspace(0, 1, 11),\n",
    "}\n",
    "\n",
    "# Set the random search hyperparameterization method parameters\n",
    "random_search_params = {\n",
    "    \"param_distributions\": param_dist,\n",
    "    \"scoring\": \"f1_weighted\",\n",
    "    \"random_state\": 0,\n",
    "    \"n_jobs\": -1,\n",
    "    \"cv\": straified_k_folds,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d37a33b",
   "metadata": {},
   "source": [
    "## Train binary logistic regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c4c0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for model_HCM (final)...\n",
      "Optimal parameters for model_HCM (final): {'l1_ratio': 0.4, 'C': 0.1}\n",
      "Model saved as: models/redo_DMSO_plate/model_HCM_final_downsample.joblib\n",
      "Training model for model_HCM (shuffled)...\n",
      "Optimal parameters for model_HCM (shuffled): {'l1_ratio': 0.4, 'C': 0.1}\n",
      "Model saved as: models/redo_DMSO_plate/model_HCM_shuffled_downsample.joblib\n",
      "Training model for model_all_hearts (final)...\n",
      "Optimal parameters for model_all_hearts (final): {'l1_ratio': 0.4, 'C': 0.1}\n",
      "Model saved as: models/redo_DMSO_plate/model_all_hearts_final_downsample.joblib\n",
      "Training model for model_all_hearts (shuffled)...\n",
      "Optimal parameters for model_all_hearts (shuffled): {'l1_ratio': 0.1, 'C': 0.31622776601683794}\n",
      "Model saved as: models/redo_DMSO_plate/model_all_hearts_shuffled_downsample.joblib\n",
      "Training model for model_DCM (final)...\n",
      "Optimal parameters for model_DCM (final): {'l1_ratio': 0.4, 'C': 0.1}\n",
      "Model saved as: models/redo_DMSO_plate/model_DCM_final_downsample.joblib\n",
      "Training model for model_DCM (shuffled)...\n",
      "Optimal parameters for model_DCM (shuffled): {'l1_ratio': 0.4, 'C': 0.1}\n",
      "Model saved as: models/redo_DMSO_plate/model_DCM_shuffled_downsample.joblib\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression and RandomizedSearchCV\n",
    "logreg = LogisticRegression(**logreg_params)\n",
    "random_search = RandomizedSearchCV(logreg, **random_search_params)\n",
    "\n",
    "# Loop through the training data dictionary\n",
    "for dataset_name, data_dict in training_data.items():\n",
    "    X_train = data_dict[\"X_train\"]\n",
    "    y_train = data_dict[\"y_train\"]\n",
    "    X_shuffled_train = data_dict[\"X_shuffled_train\"]\n",
    "    y_shuffled_train = data_dict[\"y_shuffled_train\"]\n",
    "\n",
    "    with parallel_backend(\"multiprocessing\"):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\", category=ConvergenceWarning, module=\"sklearn\"\n",
    "            )\n",
    "\n",
    "            ########################################################\n",
    "            # Train model on non-shuffled (final) training data\n",
    "            ########################################################\n",
    "            print(f\"Training model for {dataset_name} (final)...\")\n",
    "            final_random_search = clone(random_search)\n",
    "            final_random_search.fit(X_train, y_train)\n",
    "            print(\n",
    "                f\"Optimal parameters for {dataset_name} (final):\",\n",
    "                final_random_search.best_params_,\n",
    "            )\n",
    "\n",
    "            # Save model\n",
    "            final_model_filename = model_dir / f\"{dataset_name}_final_downsample.joblib\"\n",
    "            dump(final_random_search.best_estimator_, final_model_filename)\n",
    "            print(f\"Model saved as: {final_model_filename}\")\n",
    "\n",
    "            ########################################################\n",
    "            # Train model on shuffled training data\n",
    "            ########################################################\n",
    "            print(f\"Training model for {dataset_name} (shuffled)...\")\n",
    "            shuffled_random_search = clone(random_search)\n",
    "            shuffled_random_search.fit(X_shuffled_train, y_shuffled_train)\n",
    "            print(\n",
    "                f\"Optimal parameters for {dataset_name} (shuffled):\",\n",
    "                shuffled_random_search.best_params_,\n",
    "            )\n",
    "\n",
    "            # Save model\n",
    "            shuffled_final_model_filename = (\n",
    "                model_dir / f\"{dataset_name}_shuffled_downsample.joblib\"\n",
    "            )\n",
    "            dump(shuffled_random_search.best_estimator_, shuffled_final_model_filename)\n",
    "            print(f\"Model saved as: {shuffled_final_model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2635465",
   "metadata": {},
   "source": [
    "## For all_hearts only model, train a multi-class logistic regression to predict the Metadata_heart_failure_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e00fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3772, 1030)\n",
      "HCM        1419\n",
      "Healthy    1232\n",
      "DCM        1121\n",
      "Name: Metadata_heart_failure_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Update label for the model\n",
    "label = \"Metadata_heart_failure_type\"\n",
    "\n",
    "# Load in the all hearts dataset\n",
    "all_hearts_file = training_data_path / \"model_all_hearts\" / \"training_split.parquet\"\n",
    "all_hearts_df = pd.read_parquet(all_hearts_file)\n",
    "\n",
    "# Update the Metadata_heart_failure_type for NaNs to 'Healthy'\n",
    "all_hearts_df[label] = all_hearts_df[label].fillna(\"Healthy\")\n",
    "\n",
    "# Print shape and value counts for the updated column\n",
    "print(all_hearts_df.shape)\n",
    "print(all_hearts_df[label].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c13e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Class Mapping: {'DCM': 0, 'HCM': 1, 'Healthy': 2}\n"
     ]
    }
   ],
   "source": [
    "# Fit LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(all_hearts_df[label].unique())\n",
    "dump(le, encoder_dir / \"label_encoder_multi-class.joblib\")\n",
    "\n",
    "# Show class mapping\n",
    "print(\"Multi-Class Mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "# Get non-shuffled data\n",
    "X_train, y_train = get_X_y_data(df=all_hearts_df, label=label, shuffle_features=False)\n",
    "y_train_encoded = le.transform(y_train)\n",
    "\n",
    "# Get shuffled data\n",
    "X_shuffled_train, y_shuffled_train = get_X_y_data(\n",
    "    df=all_hearts_df, label=label, shuffle_features=True\n",
    ")\n",
    "y_shuffled_train_encoded = le.transform(y_shuffled_train)\n",
    "\n",
    "# Store in a single dictionary\n",
    "multi_class_training_data = {\n",
    "    \"X_train\": X_train,\n",
    "    \"y_train\": y_train_encoded,\n",
    "    \"X_shuffled_train\": X_shuffled_train,\n",
    "    \"y_shuffled_train\": y_shuffled_train_encoded,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b51004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Logistic Regression parameters for multi-class: {'penalty': 'elasticnet', 'solver': 'saga', 'max_iter': 1000, 'n_jobs': -1, 'random_state': 0, 'class_weight': 'balanced', 'multi_class': 'multinomial'}\n"
     ]
    }
   ],
   "source": [
    "# Update just logreg_params for multi-class classification\n",
    "logreg_params.update({\"multi_class\": \"multinomial\"})\n",
    "\n",
    "# Print to confirm change\n",
    "print(\"Updated Logistic Regression parameters for multi-class:\", logreg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd429b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training multi-class model for all_hearts (final)...\n",
      "Optimal parameters (final): {'l1_ratio': 0.4, 'C': 0.1}\n",
      "Model saved as: models/redo_DMSO_plate/model_all_hearts_final_multiclass.joblib\n",
      "Training multi-class model for all_hearts (shuffled)...\n",
      "Optimal parameters (shuffled): {'l1_ratio': 0.1, 'C': 0.31622776601683794}\n",
      "Model saved as: models/redo_DMSO_plate/model_all_hearts_shuffled_multiclass.joblib\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression and RandomizedSearchCV\n",
    "logreg = LogisticRegression(**logreg_params)\n",
    "random_search = RandomizedSearchCV(logreg, **random_search_params)\n",
    "\n",
    "# Extract data for the multi-class all_hearts model\n",
    "X_train = multi_class_training_data[\"X_train\"]\n",
    "y_train = multi_class_training_data[\"y_train\"]\n",
    "X_shuffled_train = multi_class_training_data[\"X_shuffled_train\"]\n",
    "y_shuffled_train = multi_class_training_data[\"y_shuffled_train\"]\n",
    "\n",
    "with parallel_backend(\"multiprocessing\"):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "\n",
    "        ########################################################\n",
    "        # Train model on non-shuffled (final) training data\n",
    "        ########################################################\n",
    "        print(\"Training multi-class model for all_hearts (final)...\")\n",
    "        final_random_search = clone(random_search)\n",
    "        final_random_search.fit(X_train, y_train)\n",
    "        print(\"Optimal parameters (final):\", final_random_search.best_params_)\n",
    "\n",
    "        # Save model\n",
    "        final_model_filename = model_dir / \"model_all_hearts_final_multiclass.joblib\"\n",
    "        dump(final_random_search.best_estimator_, final_model_filename)\n",
    "        print(f\"Model saved as: {final_model_filename}\")\n",
    "\n",
    "        ########################################################\n",
    "        # Train model on shuffled training data\n",
    "        ########################################################\n",
    "        print(\"Training multi-class model for all_hearts (shuffled)...\")\n",
    "        shuffled_random_search = clone(random_search)\n",
    "        shuffled_random_search.fit(X_shuffled_train, y_shuffled_train)\n",
    "        print(\"Optimal parameters (shuffled):\", shuffled_random_search.best_params_)\n",
    "\n",
    "        # Save model\n",
    "        shuffled_final_model_filename = (\n",
    "            model_dir / \"model_all_hearts_shuffled_multiclass.joblib\"\n",
    "        )\n",
    "        dump(shuffled_random_search.best_estimator_, shuffled_final_model_filename)\n",
    "        print(f\"Model saved as: {shuffled_final_model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fibrosis_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
